{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6288875a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Improved Real-Time trailing Stop\n",
    "\n",
    "This Final Project presents an improved real-time trailing-stop strategy. It keeps track in real time of max, min, mean, VOL (volatility), FD (fractal dimension) and the returns ($r_i$) per 6 minute interval. Additionally, the code keeps track of the balance of a currency pair at each hour and also the profit (or loss).\n",
    "\n",
    "1. The code does this by calculating the volatility as a first order calculation (max â€“ min). After that, 200 Keltner Channels are created in the next interval using the mean and the volatility from the previous interval:\n",
    "\n",
    "    $Keltner Channel Upper Band = Mean Value + n0.025VOL, \\ n \\in range(1,100)$ \n",
    "\n",
    "    $Keltner Channel Lower Band = Mean Value - n0.025VOL, \\ n \\in range(1,100)$ \n",
    "\n",
    "    The number of crosses with the Keltner bands get counted during the 6 minute interval and then the fractal dimension is calculated as: $\\frac{nb\\_cross}{VOL}$.\n",
    "   \n",
    "   Extra info: this is based on brownian montion with the Mean Value as the best approximation for the next 6 minutes and the 0.025 is the step size. \n",
    "\n",
    "\n",
    "2. The returns are calculated using the following formula:\n",
    "$r_i = \\frac{(P_i - P_{i-1})}{P_{i-1}}$ with $P_i$ the mean price in the current interval and $P_{i-1}$ the mean price in the previous interval (see HWK 3 explanation).\n",
    "\n",
    "\n",
    "3. A trade can have up to four layers. After each hour we check, if our stop loss is reached. If the position is doing well, we add 100 extra currency units to our trade until after the fourth layer then we stop buying or selling. The stop loss also decreases with each layer to protect our profits: 0.250% &rarr; 0.150% &rarr; 0.100% &rarr; 0.050%.\n",
    "\n",
    "    This stop loss gets compared with an approximation of the returns, using the last 10 return values. For example: $r_{10} = \\sum_{i=2}^{10} r_{i}$\n",
    "\n",
    "\n",
    "4. The code also keeps tracks of the average buy or sell price to be able to calculate the exact profits when closing a trade.\n",
    "\n",
    "\n",
    "5. Note that the first 6 minute checkpoint doesn't yield a FD or a return value and these are set to Null, because there is no previous information yet.\n",
    "\n",
    "\n",
    "6. The code uses a regression model to predict the return of the next hour\n",
    "\n",
    "\n",
    "7. The code also implements a sanity check to see if the provided data by Polygon makes sense and is correct and a check for when a certain pair has low liquidity (VOL=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0833fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "__Best practice 1:__ Goal is to build an optimized trailing-stop strategy to trade FOREX markets using ML techniques for prediction.\n",
    "\n",
    "Spread througout the code: \n",
    "\n",
    "   -__Best practice 2__: Collecting all fields that are relevant\n",
    "    \n",
    "   -__Best practice 3__: Maintaining the consistency of field values\n",
    "   \n",
    "   -__Best practice 4__: Dealing with missing and wrong data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc651",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52df875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from threading import Thread\n",
    "from dotenv import load_dotenv\n",
    "from datetime import timedelta\n",
    "from pymongo import MongoClient\n",
    "from pycaret.regression import *\n",
    "from keras.models import Sequential\n",
    "from polygon.rest import RESTClient  # polygon.rest allows to just install the full polygon package without any problems\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36516d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the extra classes\n",
    "import Portfolio\n",
    "import PredictionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31516e",
   "metadata": {},
   "source": [
    "## Extra Classes\n",
    "\n",
    "I chose to move the extra classes to different python files and import them to keep this notebook concise and understanable. For documentation and more information about the extra classes that I use you can open their python files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eede51a",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The functions below are helper functions used in the main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function slightly modified from polygon sample code to format the date string \n",
    "\n",
    "Arguments:\n",
    "    - ts: last_trade.timestamp (timestap from the last_trade object)\n",
    "    \n",
    "Returns:\n",
    "    - datetime object\n",
    "\"\"\"\n",
    "def ts_to_datetime(ts) -> str:\n",
    "    return datetime.datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function intializes a dictionary for the currency pair with all the parameters that we need to keep track off\n",
    "during our trades.\n",
    "\n",
    "This function also initializes a start mean and a start volatility to begin the code with, used in the sanity check. \n",
    "For the the start mean the current price is used and for the start volatility 2% is used, with the reasoning that \n",
    "it is statistically very unlikely to have such high volatility (especially in a 6 minute interval).\n",
    "\n",
    "Arguments:\n",
    "    - currency_pair: Tuple, with the currency pair\n",
    "    \n",
    "    - client: Object, Polygon REST api client\n",
    "\n",
    "Returns:\n",
    "    - Dictionary, with al the parameter to keep track off\n",
    "\"\"\"\n",
    "def initialize_dictionary(currency_pair, client, position):\n",
    "    \n",
    "    ticker = \"C:\" + currency_pair[0] + currency_pair[1]\n",
    "    last_quote = client.get_last_quote(ticker)\n",
    "    prev_mean = (last_quote.ask_price + last_quote.bid_price)/2\n",
    "    portfolio = Portfolio.Portfolio(currency_pair[0], currency_pair[1], client)\n",
    "    \n",
    "    return {\"prev_band_nb\": 0, \"nb_crosses\": 0,\"prev_vol\": 0.02, \"prev_mean\": prev_mean,\n",
    "            \"max\": 0, \"min\": 9999, \"portfolio\": portfolio, \"position\": position, \"prev_pred\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This the sanity check (best practice 4) for the returned values of Polygon. I noticed that Polygon sometimes returns an \n",
    "incorrect value that is almost exact the same as 1/correct. This makes me believe that Polygon probably sometimes switches \n",
    "the from and to currency.\n",
    "\n",
    "I use the reasoning that it is statiscally very unlikely that avg_price > prev_mean + 750*0.025*prev_vol and,\n",
    "avg_price < prev_mean - 750*0.025*prev_vol\n",
    "\n",
    "Arguments:\n",
    "    - client: Object of Polygon client\n",
    "    \n",
    "    - avg_price: Float, avg price of currency pair\n",
    "    \n",
    "    - currency_pair: Tuple, with the currency pair\n",
    "    \n",
    "    - curr_dict: Dictionary, contains all the required parameters\n",
    "\n",
    "Returns:\n",
    "    - avg_price\n",
    "\n",
    "\"\"\"\n",
    "def filter_out_incorrect_values(client, avg_price, currency_pair, curr_dict):\n",
    "    \n",
    "    if (avg_price > curr_dict[\"prev_mean\"] + 750*0.025*curr_dict[\"prev_vol\"] or avg_price < curr_dict[\"prev_mean\"] - 750*0.025*curr_dict[\"prev_vol\"]) and curr_dict[\"prev_vol\"] != 0:\n",
    "        print(\"Very unlikely value detected:\", avg_price)\n",
    "        \n",
    "        # Call the API again with the required parameters, using different call method\n",
    "        try:\n",
    "            ticker = \"C:\" + currency_pair[0] + currency_pair[1]\n",
    "            last_quote = client.get_last_quote(ticker)\n",
    "            avg_price = (last_quote.ask_price + last_quote.bid_price)/2\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if (avg_price > curr_dict[\"prev_mean\"] + 750*0.025*curr_dict[\"prev_vol\"] or avg_price < curr_dict[\"prev_mean\"] - 750*0.025*curr_dict[\"prev_vol\"] ) and curr_dict[\"prev_vol\"] != 0:\n",
    "        # Value is still wrong, discard the value\n",
    "        avg_prive = None  # None values are ignored when doing operation on NoSql database, such as AVG() + get removed in preprocessing\n",
    "    \n",
    "    return avg_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e136705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The update_cross_counter() function is used to count the number of times that a given value \"crosses\" a specific threshold. \n",
    "This threshold is calculated using an average price and a previous volume and mean value.\n",
    "\"\"\"\n",
    "def update_cross_counter(avg_price, curr_dict):\n",
    "    \n",
    "    prev_vol = curr_dict[\"prev_vol\"]\n",
    "    prev_mean = curr_dict[\"prev_mean\"]\n",
    "    # Assumption point on a band is not a cross, has to be > or <\n",
    "\n",
    "    # Keltner bands, count the number of crosses in realtime\n",
    "    prev_band_nb = curr_dict[\"prev_band_nb\"]\n",
    "\n",
    "    # Formula used to calculate the band nb:\n",
    "    # band_nb = floor((abs(avg_price - prev_mean))/(0.025*prev_vol))\n",
    "\n",
    "    if avg_price > prev_mean + 0.025*prev_vol and prev_vol != 0:\n",
    "        band_nb = floor((avg_price - prev_mean)/(0.025*prev_vol))\n",
    "\n",
    "    elif avg_price < prev_mean - 0.025*prev_vol and prev_vol != 0:\n",
    "        band_nb = floor((avg_price - prev_mean)/(-0.025*prev_vol))\n",
    "\n",
    "    else:\n",
    "        band_nb = 0  # lays within the keltner channel\n",
    "\n",
    "    # Can't go over 100 for the band_nb\n",
    "    band_nb = 100 if band_nb > 100 else band_nb\n",
    "\n",
    "\n",
    "    # Set new prev_band_nb and increment crosses counter\n",
    "    curr_dict[\"prev_band_nb\"] = band_nb\n",
    "    curr_dict[\"nb_crosses\"] = curr_dict[\"nb_crosses\"] + abs(band_nb - prev_band_nb)\n",
    "    \n",
    "    return curr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is called every 6 minutes to aggregate the data, store it in the aggregate table, and then delete the raw data\n",
    "\n",
    "Arguments:\n",
    "    - engine: Engine object from sqlalchemy\n",
    "    \n",
    "    - curr: Tuple, with the currency pair\n",
    "    \n",
    "    - curr_dict: Dictionary, contains all the required parameters\n",
    "    \n",
    "    - start_time: Float, the start time in seconds of the code\n",
    "\n",
    "Returns:\n",
    "    - None\n",
    "\n",
    "\"\"\"\n",
    "def aggregate_raw_data_tables(mongo_client, curr, curr_dict, start_time, key_alpha):\n",
    "\n",
    "\n",
    "    # Select the database that contains the collections\n",
    "    db = mongo_client[\"FOREX_currencypairs3\"]\n",
    "    \n",
    "    # Use the .aggregate() method to run the aggregation pipeline and return the result\n",
    "    result = db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_raw\"].aggregate([\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": None,\n",
    "            \"average_price\": { \"$avg\": \"$fxrate\" },\n",
    "            \"maxtick\": { \"$max\": \"$ticktime\" }\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # Result is a cursor object, what is a form of an iterable\n",
    "    grouped_document = result.next()\n",
    "    mean = grouped_document[\"average_price\"]\n",
    "            \n",
    "    # vol = (max - min)/mean\n",
    "    vol = (curr_dict[\"max\"] - curr_dict[\"min\"])/mean  # noramlizing using the mean price\n",
    "    # previous vol\n",
    "    curr_dict[\"prev_vol\"] = vol\n",
    "\n",
    "    # Calculate the returns here\n",
    "    if time.time() < start_time + 390:\n",
    "        return_ = None\n",
    "    else:\n",
    "        return_ = (mean - curr_dict[\"prev_mean\"])/curr_dict[\"prev_mean\"]\n",
    "\n",
    "    # previous mean\n",
    "    curr_dict[\"prev_mean\"] = mean\n",
    "\n",
    "    # Prevent division by 0 error and set first FD to null, do a sanity check for low liquidity, best practice 4\n",
    "    if time.time() < start_time + 390 or vol == 0:\n",
    "        frac_dem = None\n",
    "    else:\n",
    "        frac_dem = curr_dict[\"nb_crosses\"]/vol  # normalizing using vol\n",
    "        \n",
    "    # Create the documents to insert\n",
    "    document_agg = {\"inserttime\": grouped_document[\"maxtick\"], \"avgfxrate\": mean}\n",
    "    document_maxmin = {\"inserttime\": grouped_document[\"maxtick\"], \"max\": curr_dict[\"max\"], \"min\": curr_dict[\"min\"], \n",
    "                       \"VOL\": vol, \"mean\": mean, \"FD\": frac_dem, \"return\": return_}\n",
    "\n",
    "    # Insert the documents into the collections\n",
    "    db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_agg\"].insert_one(document_agg)\n",
    "    \n",
    "    if curr[0]+curr[1] != \"EURUSD\":\n",
    "        db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_maxmin\"].insert_one(document_maxmin)\n",
    "    \n",
    "    # Add the two extra features to the EUR/USD pair\n",
    "    if curr[0]+curr[1] == \"EURUSD\":\n",
    "        \n",
    "        endpoint = 'https://www.alphavantage.co/query'\n",
    "        params = {\n",
    "          'function': 'CURRENCY_EXCHANGE_RATE',\n",
    "          'from_currency': 'XAU',\n",
    "          'to_currency': 'USD',\n",
    "          'apikey': key_alpha\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response_gold = requests.get(endpoint, params=params)\n",
    "            response_oil = requests.get(f\"{endpoint}?function=GLOBAL_QUOTE&symbol=OIL&apikey={key_alpha}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Improvement: implement a sanity check for this data as well \n",
    "\n",
    "        # Parse the JSON responses\n",
    "        data_oil = response_oil.json()\n",
    "        data_gold = response_gold.json()\n",
    "\n",
    "        # Get the current oil_price and gold price\n",
    "        oil_price = data_oil[\"Global Quote\"][\"05. price\"]\n",
    "        gold_price = data_gold['Realtime Currency Exchange Rate']['5. Exchange Rate']\n",
    "\n",
    "        # Convert the gold price to a float\n",
    "        gold_price = float(gold_price)\n",
    "        \n",
    "        document_maxmin = {\"inserttime\": grouped_document[\"maxtick\"], \"max\": curr_dict[\"max\"], \"min\": curr_dict[\"min\"], \"VOL\": vol, \n",
    "                           \"mean\": mean, \"FD\": frac_dem, \"gold_price\": gold_price, \"oil_price\": oil_price, \"return\": return_}\n",
    "        \n",
    "        db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_maxmin\"].insert_one(document_maxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a150fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function which clears the raw data tables once we have aggregated the data in a 6 minute interval and resets the number of\n",
    "crosses, band_nb, max and min parameters for the Keltner Channels.\n",
    "\n",
    "Arguments:\n",
    "    - mongo_client: The MongoDB client of our NoSQL database\n",
    "    \n",
    "    - curr: Tuple, with the currency pair\n",
    "    \n",
    "    - curr_dict: Dictionary, contains all the required parameters\n",
    "    \n",
    "Returns:\n",
    "    - None\n",
    "\"\"\"\n",
    "def reset_raw_collection(mongo_client, curr, curr_dict):\n",
    "        \n",
    "    # Select the database that contains the collection you want to delete\n",
    "    db = mongo_client[\"FOREX_currencypairs3\"]\n",
    "\n",
    "    # Delete the collection\n",
    "    db.drop_collection(curr[0]+curr[1]+curr_dict[\"position\"]+\"_raw\")\n",
    "\n",
    "    # reset number of crosses, band_nb and max, min\n",
    "    # Assumption: we start each time in band nb 0, not 100% correct, but should not lead to big differences\n",
    "    curr_dict[\"nb_crosses\"] = 0\n",
    "    curr_dict[\"band_nb\"] = 0\n",
    "    curr_dict[\"max\"] = 0\n",
    "    curr_dict[\"min\"] = 9999  # min is set to a random high number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code defines a function determine_strategy_next_hour that takes in a mongo_client, curr, and curr_dict as input. \n",
    "The mongo_client is a MongoClient object that is used to connect to a MongoDB database. The curr input is a tuple containing \n",
    "the current currency pair being traded, and the curr_dict input is a dictionary containing information about the current trade \n",
    "position and portfolio.\n",
    "\n",
    "Arguments:\n",
    "    - mongo_client: The MongoDB client of our NoSQL database\n",
    "    \n",
    "    - curr: Tuple, with the currency pair\n",
    "    \n",
    "    - curr_dict: Dictionary, contains all the required parameters\n",
    "    \n",
    "Returns:\n",
    "    - Reinvest_flag, determines wether we reinvest or stay neutral on our trade position\n",
    "\"\"\"\n",
    "def determine_strategy_next_hour(mongo_client, curr, curr_dict):\n",
    "    \n",
    "    # Set reinvestflag to true\n",
    "    reinvest_flag = True\n",
    "    \n",
    "    # Get the database\n",
    "    db = mongo_client[\"FOREX_currencypairs3\"]\n",
    "    \n",
    "    # Get the needed collections for the prediction and comparision\n",
    "    collection_maxmin = db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_maxmin\"]\n",
    "    \n",
    "    # Load the LSTM model\n",
    "    predObj = PredictionModel.Predictionmodel()\n",
    "    yhat = predObj.predict_with_LSTM(mongo_client, curr, curr_dict[\"position\"])\n",
    "    \n",
    "    # Load the trained regression model\n",
    "    # loaded_model = load_model('pipeline_' + curr[0] + curr[1] + curr_dict[\"position\"])\n",
    "\n",
    "    # Get the exact hour return from the previous hour\n",
    "    result = collection_maxmin.find({},{\"return\": 1, \"_id\":0}).sort([(\"_id\", -1)]).skip(10).limit(10)\n",
    "    R10_prev_hour = sum(value[\"return\"] for value in result)\n",
    "           \n",
    "    # Make prediction using the PredictionModel class\n",
    "    # predObj = PredictionModel.PredictionModel()\n",
    "    # predObj.best_model = loaded_model\n",
    "    # result = predObj.predict_return_next_hour(mongo_client, curr, curr_dict[\"position\"], 0, 0)\n",
    "\n",
    "    # next_hour_pred = result.sum(axis=0)\n",
    "    # next_hour_pred = next_hour_pred['Label']/100000\n",
    "    \n",
    "    next_hour_pred = yhat\n",
    "    \n",
    "    # Calculate the previous error\n",
    "    prev_error = curr_dict[\"prev_pred\"] - R10_prev_hour\n",
    "    \n",
    "    # Update the previous prediction\n",
    "    curr_dict[\"prev_pred\"] = next_hour_pred\n",
    "    \n",
    "    # Correct with the error\n",
    "    corrected_pred = abs(next_hour_pred + prev_error)\n",
    "    \n",
    "    # Implement the 4 options discussed to \n",
    "    if abs(next_hour_pred) >= 0 and corrected_pred >= 0:\n",
    "        reinvest_flag = True\n",
    "    \n",
    "    elif (abs(next_hour_pred) < 0 and corrected_pred > 0) or (abs(next_hour_pred) > 0 and corrected_pred < 0):\n",
    "        reinvest_flag = False\n",
    "        \n",
    "    else:\n",
    "        realised_loss = curr_dict[\"portfolio\"].close_trade(curr_dict[\"position\"])\n",
    "        print(f\"[UPDATE], realized loss of {curr[0]}_{curr[1]} is {realised_loss}\")\n",
    "\n",
    "        # Stop the thread and stop the trade\n",
    "        raise ValueError(f\"Stop loss limit reached in window {window}\")\n",
    "        \n",
    "    return reinvest_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52664b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function reviews the position every hour to check if we should close the trade, stay neutral or reinvest.\n",
    "\"\"\"\n",
    "def review_position(mongo_client, curr, curr_dict):\n",
    "        \n",
    "    # Define the stop loss limits\n",
    "    stop_loss_limits = [0.00250, 0.00150,  0.00100, 0.00050]\n",
    "    \n",
    "    # Select the database that contains the collection we want to count\n",
    "    db = mongo_client[\"FOREX_currencypairs3\"]\n",
    "\n",
    "    # Select the collection we want to count\n",
    "    # Use the .count_documents() method to count the number of documents in the collection\n",
    "    collection = db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_maxmin\"]\n",
    "    nb_documents = collection.count_documents({})\n",
    "    \n",
    "    window = min(int(nb_documents/10), 4)\n",
    "    \n",
    "    # Use the Portfolio class to calculate the exact return at this moment\n",
    "    exact_return = curr_dict[\"portfolio\"].exact_return(curr_dict[\"position\"])\n",
    "    \n",
    "    # Second option, use an estimated return, based on the previous hour\n",
    "    # result = collection_maxmin.find({},{\"return\": 1, \"_id\":0}).sort([(\"_id\", -1)]).skip(10).limit(10)\n",
    "    # estimated_return = sum(value[\"value\"] for value in result)\n",
    "\n",
    "    if abs(exact_return) > stop_loss_limits[window-1]:\n",
    "        print(f\"[UPDATE], stop loss limit is reached for {curr[0]}_{curr[1]} at {stop_loss_limits[window-1]} and trade is closed\")\n",
    "\n",
    "        realised_loss = curr_dict[\"portfolio\"].close_trade(curr_dict[\"position\"])\n",
    "        print(f\"[UPDATE], realized loss of {curr[0]}_{curr[1]} is {realised_loss}\")\n",
    "\n",
    "        # Stop the thread and stop the trade\n",
    "        raise ValueError(f\"Stop loss limit reached in window {window}\")\n",
    "    \n",
    "    reinvest_flag = determine_strategy_next_hour(mongo_client, curr, curr_dict)\n",
    "\n",
    "    if curr_dict[\"position\"] == \"LONG\" and reinvest_flag:\n",
    "        curr_dict[\"portfolio\"].buy_curr(100)\n",
    "\n",
    "    elif curr_dict[\"position\"] == \"SHORT\" and reinvest_flag:\n",
    "        curr_dict[\"portfolio\"].sell_curr(100)\n",
    "        \n",
    "    collection_balance = db[curr[0]+curr[1]+curr_dict[\"position\"]+\"_balance\"]\n",
    "    \n",
    "    profit = curr_dict[\"portfolio\"].calculate_profit(curr_dict[\"position\"])\n",
    "    balance = curr_dict[\"portfolio\"].convert_balance(curr_dict[\"position\"])\n",
    "    exact_return = curr_dict[\"portfolio\"].exact_return(curr_dict[\"position\"])\n",
    "    \n",
    "    # Create the document to insert\n",
    "    document = {\"balance_in_USD\": balance, \"profit_loss_in_USD\": profit, \"exact_return\": exact_return}\n",
    "\n",
    "    # Insert the document into the collection\n",
    "    collection_balance.insert_one(document)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d81a11",
   "metadata": {},
   "source": [
    "## Mainfunction\n",
    "\n",
    "Each currency pair runs on a different thread to exploit parallelism and each thread will call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ccccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This main function repeatedly calls the polygon api every 1 seconds for 10 hours and stores the results.\n",
    "\n",
    "Arguments:\n",
    "    - currency_pairs: Tuple with the base and quote currency\n",
    "    \n",
    "    - duration, Integer, time in seconds for how long the trade should last\n",
    "    \n",
    "    - mongo_client: The MongoDB client of our NoSQL database\n",
    "    \n",
    "    - position_type: String, that is either \"SHORT\" or \"LONG\" depending on the type of the position\n",
    "    \n",
    "Returns:\n",
    "    - None\n",
    "\"\"\"\n",
    "def mainThread(currency_pair, mongo_client, duration, position_type):\n",
    "    \n",
    "    # API key to access Polygon\n",
    "    key_poly = os.environ.get(\"api-key\")\n",
    "    # API key to access alphavantage\n",
    "    key_alpha = os.environ.get(\"api-key2\")\n",
    "    \n",
    "    # Open a RESTClient for making the api calls\n",
    "    client = RESTClient(key_poly)\n",
    "    \n",
    "    # Create dictionary for the currency_pair with the parameters to keep track of\n",
    "    curr_dict = initialize_dictionary(currency_pair, client, position_type)\n",
    "        \n",
    "    if position_type == \"LONG\":\n",
    "        curr_dict[\"portfolio\"].buy_curr(100)\n",
    "\n",
    "    else:\n",
    "        curr_dict[\"portfolio\"].sell_curr(100)\n",
    "    \n",
    "    # Start the trading loop for the currency pair\n",
    "    start_trade_loop(client, mongo_client, currency_pair, curr_dict, position_type, duration, key_alpha)\n",
    "    \n",
    "    # After the while loop the trade gets closed and profit/loss is realised by closing the trade\n",
    "    realised_profit = curr_dict[\"portfolio\"].close_trade(position_type)\n",
    "    print(f\"[UPDATE], Closed trade of {currency_pair[0]}_{currency_pair[1]} with realized profit/loss of {realised_profit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The loop for the trading process, keeps running for the duration passed by the user or gets closed early if a stop loss is reached.\n",
    "\n",
    "Arguments:\n",
    "    - mongo_client: The MongoDB client of our NoSQL database\n",
    "    \n",
    "    - currency_pair: Tuple, with the currency pair\n",
    "    \n",
    "    - curr_dict: Dictionary, contains all the required parameters\n",
    "    \n",
    "    - currency_pairs: Tuple with the base and quote currency\n",
    "    \n",
    "    - position_type: String, that is either \"SHORT\" or \"LONG\" depending on the type of the position\n",
    "    \n",
    "    - duration, Integer, time in seconds for how long the trade should last\n",
    "    \n",
    "Returns:\n",
    "    - None\n",
    "\"\"\"\n",
    "def start_trade_loop(client, mongo_client, currency_pair, curr_dict, position_type, duration, key_alpha):\n",
    "    \n",
    "    # Create run intervals using the user specified duration\n",
    "    start_6min_interval = start_1hour_interval = start_time = time.time()\n",
    "    end_time = start_time + duration\n",
    "    \n",
    "    print(f\"[UPDATE] Currency pair {currency_pair[0]}_{currency_pair[1]} started at {time.localtime(start_time).tm_hour}h {time.localtime(start_time).tm_min}min.\")\n",
    "\n",
    "    \n",
    "    # Loop that runs until the total duration of the trade until the end time is reached\n",
    "    while time.time() < end_time:\n",
    "        \n",
    "        # Aggregate the data and clear the raw data tables\n",
    "        if time.time() > start_6min_interval + 360:\n",
    "            print(f\"[UPDATE] Currency pair {currency_pair[0]}_{currency_pair[1]} aggregated at {time.localtime(start_6min_interval + 360).tm_hour}h {time.localtime(start_6min_interval + 360).tm_min}min.\")\n",
    "            aggregate_raw_data_tables(mongo_client, currency_pair, curr_dict, start_time, key_alpha)\n",
    "            reset_raw_collection(mongo_client, currency_pair, curr_dict)\n",
    "            \n",
    "            # Start new 6 minute interval\n",
    "            start_6min_interval = time.time()\n",
    "\n",
    "        if time.time() > start_1hour_interval + 3610:\n",
    "            review_position(mongo_client, currency_pair, curr_dict)\n",
    "            \n",
    "            # Start new 1 hour interval\n",
    "            start_1hour_interval = time.time()\n",
    "        \n",
    "\n",
    "        # Set the input variables to the API\n",
    "        from_ = currency_pair[0]\n",
    "        to = currency_pair[1]\n",
    "\n",
    "        # Call the API with the required parameters\n",
    "        try:\n",
    "            resp = client.get_real_time_currency_conversion(from_, to, amount=100, precision=2)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # This gets the Last Trade object defined in the API Resource\n",
    "        last_trade = resp.last    \n",
    "\n",
    "        # Format the timestamp from the result\n",
    "        dt = ts_to_datetime(last_trade.timestamp)\n",
    "        \n",
    "        # Calculate the price by taking the average of the bid and ask prices\n",
    "        avg_price = (last_trade.bid + last_trade.ask)/2\n",
    "\n",
    "        # set needed parameters\n",
    "        curr_max = curr_dict[\"max\"]\n",
    "        curr_min = curr_dict[\"min\"]\n",
    "        \n",
    "        # Do a sanity check on the retrieved value from Polygon, best practice 4\n",
    "        avg_price = filter_out_incorrect_values(client, avg_price, currency_pair, curr_dict)\n",
    "\n",
    "        if last_trade.ask > curr_max:\n",
    "            # Update current maximum\n",
    "            curr_dict[\"max\"] = last_trade.ask\n",
    "\n",
    "        elif last_trade.bid < curr_min:\n",
    "            # Update current minimum\n",
    "            curr_dict[\"min\"] = last_trade.bid \n",
    "        \n",
    "        update_cross_counter(avg_price, curr_dict)\n",
    "\n",
    "        # Get a reference to the NoSQL database and the collection, on the first reference the database and \n",
    "        # collections get created, on first call the database and collection get created automatically \n",
    "        db = mongo_client[\"FOREX_currencypairs3\"]\n",
    "        collection = db[currency_pair[0]+currency_pair[1]+curr_dict[\"position\"]+\"_raw\"]\n",
    "\n",
    "        # Create the document to insert, best practice 2\n",
    "        document = {\"ticktime\": dt, \"fxrate\": avg_price}\n",
    "\n",
    "        # Insert the document into the collection\n",
    "        collection.insert_one(document)\n",
    "\n",
    "        # Time the code so we have approx 1 data point per second\n",
    "        time.sleep(0.980)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22143d51",
   "metadata": {},
   "source": [
    "## Training the ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc43b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curr_list = [ (\"EUR\", \"USD\"), (\"GBP\", \"USD\"), (\"USD\",\"CAD\"), (\"USD\", \"CHF\"),  (\"USD\", \"HKD\"), (\"AUD\",\"USD\"), (\"NZD\",\"USD\"), (\"USD\",\"SGD\")]\n",
    "\n",
    "# load environment variables, used to hide the mongo cluster access password and the API key\n",
    "# the .env file would not be shared to ensure the confidentially\n",
    "load_dotenv()\n",
    "\n",
    "# configure the MongoDB database\n",
    "mongo_cluster = os.environ.get(\"cluster_access\")\n",
    "mongo_client = MongoClient(mongo_cluster)\n",
    "\n",
    "for curr in curr_list:\n",
    "\n",
    "    #Before starting the trading code, train the regression models and the LSMT neural net using the training data from the previous days\n",
    "    \n",
    "    # Prediction Objects for LSMT models\n",
    "    predObj3 = PredictionModel.PredictionModel()\n",
    "    predObj4 = PredictionModel.PredictionModel()\n",
    "    \n",
    "    # Data preprocessing, best practices 3 and 4\n",
    "    dataframes_long = predObj3.data_preprocessing2(mongo_client, curr, \"LONG\")\n",
    "    dataframes_short = predObj4.data_preprocessing2(mongo_client, curr, \"SHORT\")\n",
    "\n",
    "    # sorted_datasets = predObj.data_preprocessing(mongo_client, curr, \"SHORT\")\n",
    "    \n",
    "    predObj3.train_LSMT_model(curr, dataframes_long[0], dataframes_long[1], dataframes_long[2], \"LONG\")\n",
    "    predObj3.train_LSMT_model(curr, dataframes_short[0], dataframes_short[1], dataframes_short[2], \"SHORT\")\n",
    "    \n",
    "#     result1 = predObj1.train_model(mongo_client, curr, \"LONG\")\n",
    "#     result2 = predObj2.train_model(mongo_client, curr, \"SHORT\")\n",
    "#     predObj1.best_model = result1\n",
    "#     predObj2.best_model = result2\n",
    "\n",
    "#     sorted_datasets = predObj2.data_preprocessing(mongo_client, curr, position_type)\n",
    "    \n",
    "    \n",
    "    # Saving the model, best practice 16\n",
    "#     save_model(predObj1.best_model, 'pipeline_' + curr[0] + curr[1] + \"_Regression_SHORT\")\n",
    "#     save_model(predObj2.best_model, 'pipeline_' + curr[0] + curr[1] + \"_Regression_LONG\")\n",
    "    \n",
    "    # print(f\"[UPDATE] The best model of {curr[0]}_{curr[1]} has an r2_score of {r2_score} with sorting option {predObj.sort_option}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0ab55-746a-4232-8417-4f68f3002945",
   "metadata": {},
   "source": [
    "## Start the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ad677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the database if it already exists to prevent errors.CollectionInvalid\n",
    "# Delete the database, make sure to have admin acces for this\n",
    "mongo_client.drop_database(\"FOREX_currencypairs3\")\n",
    "\n",
    "# create threads for BUY positions\n",
    "buy_list = [ (\"EUR\", \"USD\"), (\"GBP\", \"USD\"), (\"USD\",\"CAD\"), (\"USD\", \"CHF\"),  (\"USD\", \"HKD\"), (\"AUD\",\"USD\"), (\"NZD\",\"USD\"), (\"USD\",\"SGD\")]\n",
    "threads_buy = [Thread(target=mainThread, args=(pair, mongo_client, 36120, \"LONG\")) for pair in buy_list]\n",
    "\n",
    "# create threads for SHORT positions\n",
    "sell_list = [ (\"EUR\", \"USD\"), (\"GBP\", \"USD\"), (\"USD\",\"CAD\"), (\"USD\", \"CHF\"),  (\"USD\", \"HKD\"), (\"AUD\",\"USD\"), (\"NZD\",\"USD\"), (\"USD\",\"SGD\")]\n",
    "threads_sell = [Thread(target=mainThread, args=(pair, mongo_client, 36120, \"SHORT\")) for pair in sell_list]\n",
    "\n",
    "threads = threads_buy + threads_sell\n",
    "\n",
    "# Mention advantages of Threads:\n",
    "# - Fixes timing issues in the code (best practice 1)\n",
    "# - Allows for separate failures (best practice 4)\n",
    "\n",
    "# start the threads\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "    \n",
    "# wait for the threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5462c20",
   "metadata": {},
   "source": [
    "## Useful scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b9bc4",
   "metadata": {},
   "source": [
    "### Transfer SQL database into a MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite_to_mongodb(engine, mongodb_uri, mongodb_database, mongodb_collection1, mongodb_collection2):\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "\n",
    "        # Connect to the MongoDB database\n",
    "        client = pymongo.MongoClient(mongodb_uri)\n",
    "        db = client[mongodb_database]\n",
    "        collection1 = db[mongodb_collection1]\n",
    "        collection2 = db[mongodb_collection2]\n",
    "\n",
    "        # Iterate over the rows in the SQLite database\n",
    "        result = conn.execute(text('SELECT * FROM' + ' ' + mongodb_collection1))\n",
    "        for row in result:\n",
    "            # Insert the row into the MongoDB collection as a document\n",
    "            collection1.insert_one(dict(row))\n",
    "        \n",
    "        result = conn.execute(text('SELECT * FROM' + ' ' + mongodb_collection2))\n",
    "        for row in result:\n",
    "            # Insert the row into the MongoDB collection as a document\n",
    "            collection2.insert_one(dict(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2ecb6",
   "metadata": {},
   "source": [
    "### Cubic Interpolation of Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_spline_interpolation_daydata(data, handle_missing = 'interpolate', extrapolate = 0):\n",
    "    # Extract the dates and values from the input data\n",
    "    dates = [dat[0] for dat in data]\n",
    "    values = [dat[1] for dat in data]\n",
    "    \n",
    "    # Best practice 4:\n",
    "    # Handle missing data\n",
    "    if handle_missing == 'interpolate':\n",
    "        # Interpolate missing values\n",
    "        values = [value if value is not None else np.nan for value in values]\n",
    "    elif handle_missing == 'drop':\n",
    "        # Drop missing values\n",
    "        values = [value for value, date in zip(values, dates) if value is not None]\n",
    "        dates = [date for value, date in zip(values, dates) if value is not None]\n",
    "    elif handle_missing == 'fill':\n",
    "        # Fill missing values with the previous value\n",
    "        prev_value = None\n",
    "        for i, (value, date) in enumerate(zip(values, dates)):\n",
    "            if value is None:\n",
    "                values[i] = prev_value\n",
    "            else:\n",
    "                prev_value = value\n",
    "    \n",
    "    # Create the interpolation function\n",
    "    interpolation_function = InterpolatedUnivariateSpline(np.arange(0,24*(len(dates))/2,12), values, k=3)\n",
    "\n",
    "    # Interpolate the values for the output dates\n",
    "    output_values = interpolation_function(np.arange(0,24*len(dates)/2,), ext = extrapolate)\n",
    "\n",
    "    # Return the interpolated values\n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f68a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_spline_interpolation_hourlydata(values, handle_missing = 'interpolate', extrapolate = 0):\n",
    "    \n",
    "    # Data already checked in daydata function, use output of that function\n",
    "\n",
    "    # Create the interpolation function\n",
    "    interpolation_function = InterpolatedUnivariateSpline(np.arange(0,len(values)*60, 60), values, k=3)\n",
    "\n",
    "\n",
    "    # Interpolate the values for the output dates\n",
    "    output_values = interpolation_function(np.arange(0,len(values)*60 + 60, 6), ext = extrapolate)\n",
    "\n",
    "    # Return the interpolated values\n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dba740-6ca8-44ba-bd9c-273eb156a807",
   "metadata": {},
   "source": [
    "## Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89e96c",
   "metadata": {},
   "source": [
    "The first improvement would be to use the techniques dicussed in class to get a better model for the\n",
    "predictions.\n",
    "\n",
    "\n",
    "The second improvement would be to extend the conversion method in the Portfolio class to also allow\n",
    "for cross pairs to be traded.\n",
    "\n",
    "\n",
    "There is also still a bit of time lost when aggregating and resetting the raw data tables. After the code\n",
    "runs for 10 hours this results in a small delay of â‰ˆ 45s. When running the code non stop this could\n",
    "eventually lead to timing issues. The quick fix in HWK 3 was to give the code a 2 minute buffer at the\n",
    "end to finish to full 10 hour run. A better fix for this would be done by calling a separate process (thread)\n",
    "to aggregate and reset the raw data tables. This would require the code to work with 2 sets of tables:\n",
    "one set that is being activily updated, while the other set can be aggregated and reset.\n",
    "\n",
    "\n",
    "Potentially, there could also be a documentation document generated to make the code more understand\u0002able.\n",
    "\n",
    "\n",
    "Maybe add a visual display that keeps live track of the trades and gives feedback\n",
    "\n",
    "\n",
    "Do a stastical analysis of the models to see where I can still improve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
